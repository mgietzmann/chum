{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = #\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "from llama_index import (\n",
    "    load_index_from_storage, SimpleDirectoryReader, StorageContext, \n",
    "    ServiceContext, GPTVectorStoreIndex, LLMPredictor, PromptHelper\n",
    ")\n",
    "from langchain import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Idea\n",
    "\n",
    "The whole idea behind Network Earth is to provide the instrumentation updates that catalyze new and tremendous science. However a catalyst needs to bring together all the constituent parts in the right way. And that means it's not enough for us to just build the instrumentation. We also need to pull those who would use it into the equation. What we are looking for specifically are those who are at the edge of what science can do and are looking to do more but just don't quite have the tools they need to make it happen. We're looking for the trail blazers. \n",
    "\n",
    "However identifying just one or two trailblazers is not enough because technology really brings benefits when it gives us generalized solutions to problems that can then form a foundation for many things to come. Therefore we want to find trailblazers who are all more or less seeing the same things so that we can find generalized rather than bespoke solutions to problems. \n",
    "\n",
    "Finding all of these trailblazers and organizing them into groups and categories would mean an absurd amount of reading on my part so let's see if there's a way to use some of the latest and greatest tech to reduce the effort on my end so I can focus on the more creative parts of this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.indices.vector_store.base.VectorStoreIndex at 0xffff5dbdf5b0>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REPORT = 'chatbot_data'\n",
    "\n",
    "def construct_index(directory_path):\n",
    "    # set maximum input size\n",
    "    max_input_size = 4096\n",
    "    # set number of output tokens\n",
    "    num_outputs = 256\n",
    "    # set maximum chunk overlap\n",
    "    chunk_overlap_ratio = 0.1\n",
    "    # set chunk size limit\n",
    "    chunk_size_limit = 600\n",
    "\n",
    "    # define LLM\n",
    "    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\", max_tokens=num_outputs))\n",
    "    prompt_helper = PromptHelper(max_input_size, num_outputs, chunk_overlap_ratio, chunk_size_limit=chunk_size_limit)\n",
    "\n",
    "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "\n",
    "    index = GPTVectorStoreIndex(\n",
    "        documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper\n",
    "    )\n",
    "\n",
    "    index.storage_context.persist('index.json')\n",
    "\n",
    "    return index\n",
    "\n",
    "construct_index(REPORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "client = openai.OpenAI()\n",
    "\n",
    "service_context = ServiceContext.from_defaults(chunk_size=512)\n",
    "storage_context = StorageContext.from_defaults(persist_dir=f\"index.json\")\n",
    "index = load_index_from_storage(storage_context, service_context=service_context).as_query_engine()\n",
    "\n",
    "def expand_on_it(thought, client, index):\n",
    "    response = index.query(f\"Expand on {thought}\").response\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"\n",
    "        Enumerate the key points identified in the following. \n",
    "\n",
    "        {response}\n",
    "        \"\"\"\n",
    "    }]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        temperature=0.6,\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "    agent_response = response.choices[0].message.content\n",
    "    enumeration = [match for match in re.findall(r\"[0-9]+\\.([^\\n]*)\", agent_response)]\n",
    "    return enumeration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [01:07<00:00,  9.58s/it]\n"
     ]
    }
   ],
   "source": [
    "useful_thoughts = {\n",
    "    0: \"uncertainties, risks, and unknowns in stock assessments and modeling\",\n",
    "    1: \"instrumentation that would improve the quality of the stock assessment and modeling\",\n",
    "}\n",
    "\n",
    "the_thought = useful_thoughts[1]\n",
    "results = expand_on_it(the_thought, client, index)\n",
    "\n",
    "second_level = {}\n",
    "for thought in tqdm(results):\n",
    "    second_level[thought] = expand_on_it(thought, client, index)\n",
    "\n",
    "content = \"\"\n",
    "for thought, sub_thoughts in second_level.items():\n",
    "    content += f\"On {thought}: \\n\"\n",
    "    for sub_thought in sub_thoughts:\n",
    "        content += f\" - {sub_thought}\\n\"\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": f\"\"\"\n",
    "    In 500 words or less summarize the following. Try to be as consise as possible without losing any of the specifics.\n",
    "\n",
    "    {content}\n",
    "    \"\"\"\n",
    "}]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=messages,\n",
    "    temperature=0.6,\n",
    "    max_tokens=1000,\n",
    ")\n",
    "report = response.choices[0].message.content\n",
    "with open(f'{REPORT}.txt', 'w') as fh:\n",
    "    fh.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
